{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader,PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(directory_path):\n",
    "    loader = DirectoryLoader(directory_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_docs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data\\\\a16-ahn.pdf', 'page': 10}\n",
      "16:10 J. Ahn et al.\n",
      "Our system uses a commercial image labeling Web service, called IQEngines\n",
      "[IQEngines 2013], to determine the user’s initial starting position when using our\n",
      "grocery shopping application. IQEngines uses a combination of computer vision and\n",
      "crowdsourcing to tag a photo with a label describing the content of the image. For\n",
      "example, an image of a box of Frosted Cheerios cereal might be labeled “General Mills\n",
      "Frosted Cheerios”. When an image is submitted to IQEngines, the image is ﬁrst pro-\n",
      "cessed by a computer vision system in an effort to provide an accurate label. If the\n",
      "computer vision system cannot identify the image, then IQEngines passes the image to\n",
      "its crowdsourcing network for analysis and tagging. According to IQEngines, the time\n",
      "to return a label for an image varies from a few seconds for the computer vision sys-\n",
      "tem, to a few minutes for the crowdsourcing system. To ensure fast image labeling in\n",
      "our experiments, we have pretrained IQEngines with speciﬁed images and associated\n",
      "labels for each of the food items in our test environment.\n",
      "To locate a user within the indoor shopping environment, our mobile application\n",
      "prompts the user to take a picture of the nearest food item using the smartphone. After\n",
      "this image is submitted to our cloud server, the server submits the image to IQEngines\n",
      "for labeling. Upon receiving the item label for the image, our server looks up the loca-\n",
      "tion for this item using a spatial database. This spatial database contains the name,\n",
      "location, and other associated metadata for each item found in the shopping environ-\n",
      "ment. In our grocery shopping application, the coordinate system for item locations\n",
      "is expressed using the following dimensions: aisle number, aisle side (left or right),\n",
      "division number, shelf number, and item sequence number. Based on our conversations\n",
      "with local supermarkets, we have found that this coordinate system is representative\n",
      "of item databases found at some establishments. In this coordinate system, aisles are\n",
      "separated into 4-foot divisions, and shelves in each aisle are numbered from bottom\n",
      "to top. Items in each location speciﬁed by a tuple of “aisle number, aisle side, division\n",
      "number, shelf number” are ordered according to item sequence number. Figure 2 shows\n",
      "a graphical representation of this coordinate system for a typical grocery store aisle.\n",
      "4.2. Localizing the User within the Grocery Aisle\n",
      "As mentioned earlier, our approach is to apply prior work in personalized pedometry\n",
      "to the aisle navigation problem. However, some adaptation is needed for the grocery\n",
      "store scenario. We observed shoppers’ behavioral patterns as they used our application\n",
      "in grocery stores. We found that the users did not always hold the phone consistently,\n",
      "upright, pointed forward down the aisle. The mobile phone’s orientation was often\n",
      "changed, whenever they moved towards food items recommended by the AR tags of\n",
      "our application. When they moved towards the products they wished to purchase, they\n",
      "usually changed the mobile phone’s orientation, such as holding it down by their side\n",
      "while walking or in a strange angle while holding a basket or operating a cart. When-\n",
      "ever this happened, the accelerometer sensor incorrectly detected a stride. To avoid\n",
      "these false strides, we modiﬁed the pedometry algorithm to ignore sudden changes in\n",
      "acceleration if they also corresponded with sudden changes in the orientation sensor.\n",
      "Another modiﬁcation we made to the pedometry algorithm was to limit motion to\n",
      "the component in the direction parallel to the long axis of the aisle. This is essentially\n",
      "1D map matching, wherein the walls of the aisle form a map that conﬁnes the travels\n",
      "of the user to a set of acceptable paths, or in this case a single path. In this way, our\n",
      "algorithm cannot misestimate the user as being located within a shelf/wall, and thus\n",
      "our location error is limited to lie only along the long axis of the aisle.\n",
      "To achieve this, we construct a bounding box around each aisle, where we bound\n",
      "the range of the x-axis by the width of a regular aisle in the grocery. When the user\n",
      "approaches the edge of the bounding box, for example, the shelves, then we only take\n",
      "the component of the motion along the axis parallel to the bounding edge, and ignore\n",
      "ACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 16, Publication date: October 2015.\n"
     ]
    }
   ],
   "source": [
    "print(doc[10].metadata)\n",
    "print(doc[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data\\\\a16-ahn.pdf', 'page': 3, 'start_index': 0}, page_content='Supporting Healthy Grocery Shopping via Mobile Augmented Reality 16:3\\nAR tag grows in size. The tags when clicked reveal nutritional information about the\\nproduct. The tags are also colored, for example, green to indicate products that are\\nnutritionally preferable (e.g., low-calorie, gluten-free), and red to indicate products to\\navoid (e.g., high cholesterol or peanut content). Further, shoppers can specify health\\nproﬁles which may impact their food purchase choices, such as weight control, heart\\ndisease, food allergies, etc. The recommended products shown via AR tags will then\\nchange depending on what health condition/concern is indicated by the user. We believe\\nour system is the ﬁrst to integrate augmented reality tagging and pedometry-based\\nlocalization with a back-end server to provide health-based grocery recommendations\\nat the point-of-purchase. We evaluated the effectiveness of our system in a real gro-')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = make_chunks(doc)\n",
    "chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data\\\\a16-ahn.pdf', 'page': 3, 'start_index': 754}, page_content='localization with a back-end server to provide health-based grocery recommendations\\nat the point-of-purchase. We evaluated the effectiveness of our system in a real gro-\\ncery store aisle with 15 actual grocery shopping subjects to determine how easy and\\nfast the subjects reported it was to locate healthy food products and avoid unhealthy\\nones, using AR tagging with our application. We also evaluated our application’s func-\\ntionality and performance by analyzing data we collected from 104 online application\\ndemonstration/survey participants.\\n2. RELATED WORK\\nAugmented reality has been recently applied in the mobile health arena in a variety\\nof applications. For example, AR tags are overlaid in a home environment to pro-\\nvide instructions to the elderly for tasks like taking medication, cooking, washing,\\netc. [Herv´as et al. 2011]. TriggerHunter is an AR-based game that overlays tags on\\npotential asthma triggers in the local environment [Hong et al. 2010]. Neither project')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import HuggingFaceEmbeddings,OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\n",
    "# import chromadb\n",
    "# from chromadb.utils import embedding_functions\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(chunks, embedding_function,CHROMA_PATH=\"chroma_db\"):\n",
    "    chroma = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        collection_name=\"my_collection\"\n",
    "    )\n",
    "    # chroma.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Chroma database path\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "\n",
    "# Initialize Hugging Face embedding\n",
    "hugging_face_ef = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_TOKEN,\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 162 chunks to chroma_db.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_22236\\452922040.py:8: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  chroma.persist()\n"
     ]
    }
   ],
   "source": [
    "# save_embeddings(chunks, hugging_face_ef,CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_docs(query,k=4):\n",
    "    # load db\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=hugging_face_ef,\n",
    "        collection_name=\"my_collection\"\n",
    "    )\n",
    "        \n",
    "    # search db\n",
    "    results = db.similarity_search_with_relevance_scores(query,k=k)\n",
    "    \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get query input\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "result = get_top_docs(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'page': 2, 'source': 'data\\\\a16-ahn.pdf', 'start_index': 2301}, page_content='by the mobile camera and overlays the result on top of the text [WordLens 2014]; and\\nmany more.\\nA prototype of our augmented reality mobile grocery shopping application is shown\\nin Figure 1. As the user pans and walks up and down a grocery store aisle, the AR\\ntags corresponding to highlighted products will change based on what products the\\nuser is facing. As a user walks towards an item along the aisle, its corresponding\\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 12, No. 1s, Article 16, Publication date: October 2015.'),\n",
       "  0.5298927030103356),\n",
       " (Document(metadata={'page': 1, 'source': 'data\\\\review_paper.pdf', 'start_index': 1613}, page_content='have accumulated from 104 participants of our online demonstration/survey.\\nReview\\nThere are some works regarding shopping assistant that has done by other researchers. However, there are\\nstill many challenges and limitation exist in their works. Hence, several relevant methods regarding\\nshopping in AR will be reviewed including the limitations and some proposed solutions.\\nA. AR Frameworks in the Food Industry\\nThere are numerous sources that detail multiple AR frameworks, like Vuforia and ARCore and MAXST, and\\nusage of the tool while going for grocery shopping in such an industry.\\nSource \\n[1]\\n \\ncompares the functionality of the three frameworks for creation of a mobile AR grocery shopping\\napp. Here, it scans a product and gives information and shows them on the smartphone screen. This paper\\nis a performance evaluation for target recognition and tracking in the real world, wherein a grocery store'),\n",
       "  0.5155333396849432),\n",
       " (Document(metadata={'page': 2, 'source': 'data\\\\review_paper.pdf', 'start_index': 719}, page_content='experiences in shopping are made. AR can be integrated into almost all aspects of the food supply chain for\\neﬃciency, safety, and innovation.\\n \\nB. Target Recognition and Tracking in AR\\nThe AR enriches the grocery shop consumer experience by projecting virtual content including nutritional\\ninformation, recommendations, and interactive displays onto real-world objects, Using OCR tools and AR\\nframeworks like ARCore and ARFusion. It, therefore, has to exploit the most advanced beneﬁts of target\\nrecognition and tracking capabilities in overlaying adequate content. It is required to detect and track the\\ntarget objects under dynamically changing conditions such as grocery stores whose lighting conditions,\\nplacements of products or movements of users can aﬀect its performance\\n \\n[2, 3, 4]\\n.\\nThe functionalities and performance of target recognition and tracking capabilities are diﬀerent in every AR\\nframework. Some very popular ones are Vuforia, ARCore, and MAXST \\n[2]'),\n",
       "  0.5153535946692724),\n",
       " (Document(metadata={'page': 9, 'source': 'data\\\\review_paper.pdf', 'start_index': 1771}, page_content='Appendices\\nCHALLENGES \\n• Real World Performance Limitations. Grocery stores oﬀer a very challenging real world in which AR\\nframeworks operate, including product density, shelf guardrails and lighting conditions. These may limit\\nrecognition distance; therefore, the possibility of distinguishing unique products at greater distances is\\nreduced and may also make it diﬃcult to recognize multiple instances of product simultaneously. \\n• Misidentiﬁcation and Display Issues: Misidentiﬁcation appears to occur often with AR systems due to\\nsimilar appearances in packaging. Thus, displays are incorrect information. Again, when the consumer\\ndisplay AR information within cluttered shop environments, overlapping or obscured views may prevent the\\nuser from seeing and picking up products correctly. \\n• Intentions do not correlate with behavior: Food scanner apps may impact intentions to make healthier'),\n",
       "  0.5152634627647092)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\RAG using langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question given based only on the context given below.\n",
    "context:\n",
    "{context}\n",
    "\n",
    "---\n",
    "based on the context above answer the following query:\n",
    "{query}\n",
    "\"\"\"\n",
    "    top_3_results = get_top_docs(query,k=3)\n",
    "    # if top result is not releavent that is score less than 0.6\n",
    "    if top_3_results[0][1] < 0.6:\n",
    "        print(\"Query is not relevant to the provided material. So the output might not be accurate.\")\n",
    "    \n",
    "    context = \"\\n\".join([result[0].page_content for result in top_3_results])\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context,query=query)\n",
    "    response = model.generate_content(prompt)\n",
    "    print(\"Query:\",query)\n",
    "    print(\"Response:\",response.text)\n",
    "    print(\"Sources:\" + \",\".join([result[0].metadata['source'] + f\" [Page:{int(result[0].metadata['page']) + 1}]\" if 'page' in result[0].metadata else \"\" for result in top_3_results]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query is not relevant to the provided material. So the output might not be accurate.\n",
      "Query: how ar affect consumer behaivour\n",
      "Response: Based on the provided text, Augmented Reality (AR) shows potential for influencing consumer behavior toward healthier food choices by offering personalized suggestions and information about products.  However, studies reveal that its effectiveness is lower than traditional methods like Front-of-Package (FOP) labeling.  While AR apps may increase intentions to buy healthier products in hypothetical situations, this doesn't always translate to actual behavior in real-world shopping scenarios.  The complexity and ease of use of AR applications are significant hurdles to overcome to maximize their impact on consumer behavior.\n",
      "\n",
      "Sources:data\\review_paper.pdf[Page:7],data\\review_paper.pdf[Page:6],data\\review_paper.pdf[Page:6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generate_response(input())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
